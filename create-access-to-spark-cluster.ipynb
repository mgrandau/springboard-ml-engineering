{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Test Access to a Spark cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'j-3TIAKNVYK1MIK'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "import json \n",
    "\n",
    "cluster_info_string_list = !aws emr create-cluster --auto-scaling-role EMR_AutoScaling_DefaultRole --applications Name=Spark Name=Livy Name=Hive Name=JupyterEnterpriseGateway --tags 'division=MSD' 'environment=Non-Prod' 'AccountID=eto820816420149' 'application=at-research' 'compliance=' 'account owner=Mark Grandau' 'costcenter=MA5150' 'AccountName=molspec' 'Name=ec2-spark-cluster' 'group=AIG' --ebs-root-volume-size 100 --ec2-attributes '{\"KeyName\":\"spark-cluster-key-pair\",\"InstanceProfile\":\"EMR_EC2_DefaultRole\",\"ServiceAccessSecurityGroup\":\"sg-0097f1805ac2a9297\",\"SubnetId\":\"subnet-e2fcc195\",\"EmrManagedSlaveSecurityGroup\":\"sg-07e0fc489417c4071\",\"EmrManagedMasterSecurityGroup\":\"sg-0bd43c1510ce6c544\",\"AdditionalMasterSecurityGroups\":[\"sg-7b798a1c\"]}' --service-role EMR_DefaultRole --release-label emr-6.3.0 --name 'spark-cluster' --instance-groups '[{\"InstanceCount\":1,\"EbsConfiguration\":{\"EbsBlockDeviceConfigs\":[{\"VolumeSpecification\":{\"SizeInGB\":32,\"VolumeType\":\"gp2\"},\"VolumesPerInstance\":2}]},\"InstanceGroupType\":\"MASTER\",\"InstanceType\":\"m5.xlarge\",\"Name\":\"Master Instance Group\"},{\"InstanceCount\":1,\"EbsConfiguration\":{\"EbsBlockDeviceConfigs\":[{\"VolumeSpecification\":{\"SizeInGB\":32,\"VolumeType\":\"gp2\"},\"VolumesPerInstance\":2}]},\"InstanceGroupType\":\"CORE\",\"InstanceType\":\"m5.xlarge\",\"Name\":\"Core Instance Group\"}]' --configurations '[{\"Classification\":\"livy-conf\",\"Properties\":{\"livy.server.session.timeout\":\"2h\"}}]' --scale-down-behavior TERMINATE_AT_TASK_COMPLETION --region us-west-2\n",
    "cluster_info_string = ''.join(cluster_info_string_list)\n",
    "\n",
    "cluster_info = json.loads(f'{cluster_info_string}')\n",
    "\n",
    "cluster_info[\"ClusterId\"]\n",
    "\n",
    "print('Wait for the cluster to finish initailizing (https://us-west-2.console.aws.amazon.com/elasticmapreduce/home?region=us-west-2#cluster-list:)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the magic has been configured to access the AWS EMR cluster.  The cluster is the Spark server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"kernel_python_credentials\": {\n",
      "    \"username\": \"Livy\",\n",
      "    \"password\": \"\",\n",
      "    \"url\": \"http://ip-172-31-43-17.us-west-2.compute.internal:8998\",\n",
      "    \"auth\": \"None\"\n",
      "  },\n",
      "  \"logging_config\": {\n",
      "    \"version\": 1,\n",
      "    \"formatters\": {\n",
      "      \"magicsFormatter\": {\n",
      "        \"format\": \"%(asctime)s\\t%(levelname)s\\t%(message)s\",\n",
      "        \"datefmt\": \"\"\n",
      "      }\n",
      "    },\n",
      "    \"handlers\": {\n",
      "      \"magicsHandler\": {\n",
      "        \"class\": \"hdijupyterutils.filehandler.MagicsFileHandler\",\n",
      "        \"formatter\": \"magicsFormatter\",\n",
      "        \"home_path\": \"~/.sparkmagic\"\n",
      "      }\n",
      "    },\n",
      "    \"loggers\": {\n",
      "      \"magicsLogger\": {\n",
      "        \"handlers\": [\n",
      "          \"magicsHandler\"\n",
      "        ],\n",
      "        \"level\": \"DEBUG\",\n",
      "        \"propagate\": 0\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"authenticators\": {\n",
      "    \"Kerberos\": \"sparkmagic.auth.kerberos.Kerberos\",\n",
      "    \"None\": \"sparkmagic.auth.customauth.Authenticator\",\n",
      "    \"Basic_Access\": \"sparkmagic.auth.basic.Basic\"\n",
      "  },\n",
      "  \"wait_for_idle_timeout_seconds\": 15,\n",
      "  \"livy_session_startup_timeout_seconds\": 60,\n",
      "  \"fatal_error_suggestion\": \"The code failed because of a fatal error:\\n\\t{}.\\n\\nSome things to try:\\na) Make sure Spark has enough available resources for Jupyter to create a Spark context.\\nb) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\\nc) Restart the kernel.\",\n",
      "  \"ignore_ssl_errors\": true,\n",
      "  \"session_configs\": {\n",
      "    \"driverMemory\": \"1000M\",\n",
      "    \"executorCores\": 2\n",
      "  },\n",
      "  \"use_auto_viz\": true,\n",
      "  \"coerce_dataframe\": true,\n",
      "  \"max_results_sql\": 2500,\n",
      "  \"pyspark_dataframe_encoding\": \"utf-8\",\n",
      "  \"heartbeat_refresh_seconds\": 30,\n",
      "  \"livy_server_heartbeat_timeout_seconds\": 0,\n",
      "  \"heartbeat_retry_seconds\": 10,\n",
      "  \"server_extension_default_kernel_name\": \"pysparkkernel\",\n",
      "  \"custom_headers\": {},\n",
      "  \"retry_policy\": \"configurable\",\n",
      "  \"retry_seconds_to_sleep_list\": [\n",
      "    0.2,\n",
      "    0.5,\n",
      "    1,\n",
      "    3,\n",
      "    5\n",
      "  ],\n",
      "  \"configurable_retry_policy_max_retries\": 8\n",
      "}"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "!cat /etc/sparkmagic/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DON'T DO THE FOLLOWING AFTER THE KERNEL RESTART\n",
    "\n",
    "Your going to want to put into any notebook that needs to connect to the spark server the cell bellow, where you hard code to the name of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read emr cluster(j-3TIAKNVYK1MIK) details\n",
      "SparkMagic config file location: /etc/sparkmagic/config.json\n",
      "Completed setting up configuration files for SparkMagic to connect to EMR cluster j-3TIAKNVYK1MIK\n",
      "\n",
      "\n",
      "\u001b[93mTo complete the setup, follow these steps:\n",
      "1. Restart the kernel. This is required so that SparkMagic can pickup the generated configuration\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "!sm-sparkmagic connect --cluster-id {cluster_info[\"ClusterId\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm that you have connection to the Spark server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1000M', 'executorCores': 2, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "PySpark (SparkMagic)",
   "language": "python",
   "name": "pysparkkernel__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sparkmagic-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
